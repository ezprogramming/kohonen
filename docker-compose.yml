services:
  # MLflow tracking server
  mlflow:
    build: .
    container_name: kohonen-mlflow
    command: mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root /app/mlflow_data/artifacts
    ports:
      - "5050:5000"
    volumes:
      - mlflow_data:/app/mlflow_data
    environment:
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-/app/mlflow_data}
    healthcheck:
      test: curl --fail http://localhost:5000 || exit 1
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Training service (designed to exit when done)
  train:
    build: .
    container_name: kohonen-train
    depends_on:
      mlflow:
        condition: service_healthy
    volumes:
      - mlflow_data:/app/mlflow_data
    env_file:
      - .env
    environment:
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5000}
      - RUN_ID_FILE=${RUN_ID_FILE:-/app/mlflow_data/run_id.txt}
    command: >
      sh -c 'echo "This container is intended to run training via make train command and exit."
             sleep 1
             exit 0'
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    restart: "no"

  # Demos service (designed to exit when done)
  demos:
    build: .
    container_name: kohonen-demos
    depends_on:
      mlflow:
        condition: service_healthy
    volumes:
      - mlflow_data:/app/mlflow_data
      - ./examples/comparison:/app/examples/comparison
    env_file:
      - .env
    environment:
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5000}
    command: >
      sh -c 'echo "This container is intended to run demos via make docker-demo-* commands and exit."
             sleep 1
             exit 0'
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    restart: "no"

  # API service
  api:
    build: .
    container_name: kohonen-api
    ports:
      - "${PORT:-8000}:${PORT:-8000}"
    depends_on:
      mlflow:
        condition: service_healthy
    volumes:
      - mlflow_data:/app/mlflow_data
    env_file:
      - .env
    environment:
      - PORT=${PORT:-8000}
      - MLFLOW_TRACKING_URI=${MLFLOW_TRACKING_URI:-http://mlflow:5000}
      - SOM_RUN_ID_FILE=${SOM_RUN_ID_FILE:-/app/mlflow_data/run_id.txt}
      - SOM_METRIC_KEY=${SOM_METRIC_KEY:-quantization_error}
      - SOM_METRIC_ASCENDING=${SOM_METRIC_ASCENDING:-true}
      - SOM_FORCE_BEST=${SOM_FORCE_BEST:-false}
    healthcheck:
      test: curl --fail http://localhost:${PORT:-8000}/health || exit 1
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 5s
    restart: unless-stopped
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    command: >
      sh -c 'if [ "${SOM_FORCE_BEST:-false}" = "true" ]; then
             python -m kohonen.scripts.api_script --force-best;
             else
             python -m kohonen.scripts.api_script;
             fi'

volumes:
  mlflow_data:
    name: kohonen_mlflow_data
    driver: local 